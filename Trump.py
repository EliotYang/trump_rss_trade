# -*- coding: utf-8 -*-
import configparser
import json
import logging
import time
import requests
import feedparser
import os
import re
import datetime
import hashlib
from typing import Any, Dict, Optional, List
from zoneinfo import ZoneInfo  # Python 3.9+
import email.utils
import calendar
from dataclasses import dataclass
import html
import unicodedata
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

# å°è¯•ä½¿ç”¨ BeautifulSoup æå‡ HTML æ¸…æ´—è´¨é‡ï¼›æ²¡æœ‰åˆ™å›é€€åˆ°æ­£åˆ™
try:
    from bs4 import BeautifulSoup  # pip install beautifulsoup4
    _HAS_BS4 = True
except Exception:
    _HAS_BS4 = False

# =========================
# æ—¥å¿—åˆå§‹åŒ–ï¼ˆé¿å…é‡å¤ handlersï¼‰
# =========================
def init_logging(log_file: str, log_level: str = "INFO"):
    level = getattr(logging, log_level.upper(), logging.INFO)
    root = logging.getLogger()
    if root.handlers:
        root.setLevel(level)
        return
    logging.basicConfig(
        level=level,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler()
        ]
    )

# =========================
# å·¥å…·ï¼šè¯»å–å¤šè¡Œå­—æ®µ
# =========================
def _cfg_get_multiline(cfg: configparser.ConfigParser, section: str, option: str, fallback: str = "") -> str:
    if cfg.has_option(section, option):
        return cfg.get(section, option).strip()
    return fallback.strip()

# =========================
# æ–‡æœ¬è§„èŒƒåŒ– / HTML æ¸…æ´— / URL æ ‡å‡†åŒ–ï¼ˆæ–°å¢ï¼‰
# =========================
def _normalize_unicode(s: str) -> str:
    """Unicode è§„èŒƒåŒ–ï¼Œé™ä½æ ‡ç‚¹/ç©ºæ ¼å˜ä½“å¯¼è‡´çš„è¯¯å·®"""
    return unicodedata.normalize("NFKC", s or "")

_TAG_RE = re.compile(r"<[^>]+>")
_WS_RE = re.compile(r"\s+")

def _strip_html_keep_text(s: str) -> str:
    """
    å°† HTML â†’ çº¯æ–‡æœ¬ï¼š
    1) html.unescape åè½¬ä¹‰å®ä½“
    2) é¦–é€‰ BeautifulSoup å»æ ‡ç­¾ï¼›è‹¥ä¸å¯ç”¨é€€åŒ–åˆ°æ­£åˆ™
    3) åˆå¹¶å¤šä½™ç©ºç™½
    4) NFKC è§„èŒƒåŒ–
    """
    if not s:
        return ""
    s = html.unescape(s)
    if _HAS_BS4:
        try:
            text = BeautifulSoup(s, "html.parser").get_text(separator=" ", strip=True)
        except Exception:
            text = _TAG_RE.sub(" ", s)
    else:
        text = _TAG_RE.sub(" ", s)
    text = _WS_RE.sub(" ", text).strip()
    return _normalize_unicode(text)

def _best_entry_body(entry) -> str:
    """
    é€‰æ‹©æ­£æ–‡çš„â€œæœ€ä½³å¯ç”¨å­—æ®µâ€ï¼š
    content[].value â†’ summary â†’ description â†’ ""
    å¹¶åš HTML â†’ çº¯æ–‡æœ¬æ¸…æ´—
    """
    # content
    if hasattr(entry, "content"):
        try:
            contents = entry.content or []
            for c in contents:
                v = c.get("value") if isinstance(c, dict) else getattr(c, "value", None)
                v = _strip_html_keep_text(v)
                if v:
                    return v
        except Exception:
            pass
    # summary
    if hasattr(entry, "summary"):
        v = _strip_html_keep_text(getattr(entry, "summary", "") or "")
        if v:
            return v
    # description
    if hasattr(entry, "description"):
        v = _strip_html_keep_text(getattr(entry, "description", "") or "")
        if v:
            return v
    return ""

def _normalize_link(url: Optional[str]) -> Optional[str]:
    """
    URL æ ‡å‡†åŒ–ï¼š
    - å»æ‰ #fragment
    - è¿‡æ»¤ utm_* ç­‰è¿½è¸ªå‚æ•°
    - ç»Ÿä¸€åŸŸåå°å†™
    - å»æ‰å°¾éšæ–œæ ï¼ˆä¿ç•™æ ¹ '/'ï¼‰
    """
    if not url:
        return None
    try:
        p = urlparse(url)
        fragment = ""
        q = []
        for k, v in parse_qsl(p.query, keep_blank_values=True):
            if k.lower().startswith("utm_"):
                continue
            q.append((k, v))
        query = urlencode(q, doseq=True)
        path = p.path.rstrip("/") or "/"
        normalized = urlunparse((p.scheme, p.netloc.lower(), path, p.params, query, fragment))
        return normalized
    except Exception:
        return url

_WEAK_TITLE_RE = re.compile(r"^\[?No Title\]? - Post from", re.IGNORECASE)

def entry_text_for_llm(entry) -> str:
    """
    æ„é€ é€æ¨¡å‹çš„â€œå‡€æ–‡æœ¬â€ï¼šæ ‡é¢˜ï¼ˆè‹¥éç³»ç»Ÿå ä½ï¼‰ + æ­£æ–‡
    """
    title_raw = getattr(entry, "title", "") or ""
    title = _strip_html_keep_text(title_raw)
    weak_title = bool(_WEAK_TITLE_RE.match(title))
    body = _best_entry_body(entry)

    if title and not weak_title:
        if body:
            return f"{title}\n{body}"
        return title
    return body

def is_effectively_empty(text: str, min_len: int = 8) -> bool:
    """æ¸…æ´—åçš„å¯è§æ–‡æœ¬é•¿åº¦æ˜¯å¦è¿‡çŸ­ï¼ˆé»˜è®¤ <8 è§†ä¸ºç©ºï¼‰"""
    if not text:
        return True
    return len(text.strip()) < min_len

def stable_fingerprint(entry) -> str:
    """
    ç¨³å®šæŒ‡çº¹ï¼šä¼˜å…ˆ id/guid/linkï¼ˆæ ‡å‡†åŒ– URLï¼‰â†’ å›é€€ title+pubDate â†’ æœ€å str(entry)
    å¯¹åŸæ–™å…ˆåš Unicode è§„èŒƒåŒ–ï¼ŒURL æ ‡å‡†åŒ–ï¼Œé™ä½è¯¯åˆ¤/æ¼åˆ¤ã€‚
    """
    parts: List[str] = []

    cand = None
    for k in ("id", "guid", "link"):
        if hasattr(entry, k) and getattr(entry, k):
            v = str(getattr(entry, k))
            if k == "link":
                v = _normalize_link(v) or v
            cand = v
            break
    if cand:
        parts.append(_normalize_unicode(cand))
    else:
        title = _normalize_unicode(getattr(entry, "title", "") or "")
        pub = ""
        for k in ("published", "pubDate", "updated", "created", "dc_date"):
            if hasattr(entry, k) and getattr(entry, k):
                pub = _normalize_unicode(str(getattr(entry, k)))
                break
        if title:
            parts.append(title)
        if pub:
            parts.append(pub)
        if not parts:
            parts.append(_normalize_unicode(str(entry)))

    raw = "||".join(parts)
    return hashlib.sha256(raw.encode("utf-8", errors="ignore")).hexdigest()

# =========================
# æ¨é€æŠ½è±¡ä¸å®ç°
# =========================
class Pusher:
    def push(self, message: str, title: Optional[str] = None, tags: Optional[str] = None):
        raise NotImplementedError

class ServerChanSDKPusher(Pusher):
    def __init__(self, send_key: str, session: requests.Session,
                 default_title: str = "ç‰¹æœ—æ™®æ¨æ–‡é¢„è­¦",
                 default_tags: str = "æœåŠ¡å™¨æŠ¥è­¦|å›¾ç‰‡",
                 max_title_len: int = 64,
                 max_desp_len: int = 512,
                 uid_override: Optional[str] = None):
        self.send_key = send_key
        self.session = session
        self.default_title = default_title
        self.default_tags = default_tags
        self.MAX_TITLE_LEN = max_title_len
        self.MAX_DESP_LEN = max_desp_len

        if uid_override:
            self.uid = uid_override
        else:
            m = re.search(r"sctp(\d+)t", send_key or "")
            self.uid = m.group(1) if m else None
            if not self.uid:
                logging.error("send_key æ ¼å¼ä¸åŒ¹é…ä¸”æœªæä¾› serverchan.uidï¼Œæ— æ³•æ¨é€")

    def push(self, message: str, title: Optional[str] = None, tags: Optional[str] = None):
        if not self.uid:
            logging.error("æœªèƒ½æå– uid æ— æ³•æ¨é€")
            return
        safe_title = (title or self.default_title)[: self.MAX_TITLE_LEN]
        safe_desp = (message or "")[: self.MAX_DESP_LEN]
        safe_tags = tags or self.default_tags

        url = f"https://{self.uid}.push.ft07.com/send/{self.send_key}.send"
        payload = {"title": safe_title, "desp": safe_desp, "tags": safe_tags}
        headers = {"Content-Type": "application/json"}
        try:
            resp = self.session.post(url, headers=headers, json=payload, timeout=10)
            print(resp.text)  # åŸå§‹è¿”å›ä¾¿äºæ’é”™
            if resp.status_code != 200:
                logging.error(f"ServerChan API æ¨é€å¤±è´¥: {resp.text}")
        except Exception as e:
            logging.error(f"ServerChan API æ¨é€å¤±è´¥: {e}")

class PusherFactory:
    @staticmethod
    def create_pusher(method: str, send_key: str, session: requests.Session,
                      title: str, tags: str, max_title_len: int, max_desp_len: int, uid_override: Optional[str]):
        if method == "serverchan_sdk" and send_key:
            return ServerChanSDKPusher(send_key, session, title, tags, max_title_len, max_desp_len, uid_override)
        else:
            logging.error("å½“å‰ä»…æ”¯æŒ serverchan_sdk æ¨é€æ–¹å¼")
            return None

# =========================
# ç¨³å¥ JSON è§£æä¸æ ¡éªŒ
# =========================
ALLOWED_IMPORTANCE = {"high", "medium", "low", "none"}
IMPORTANCE_ORDER = {"none": 0, "low": 1, "medium": 2, "high": 3}

def validate_result(obj: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    if not isinstance(obj, dict):
        return None
    impact = obj.get("impact", None)
    if not isinstance(impact, bool):
        return None
    importance = str(obj.get("importance", "none")).lower().strip()
    if importance not in ALLOWED_IMPORTANCE:
        return None
    asset_class = obj.get("asset_class", [])
    if asset_class is None:
        asset_class = []
    if not isinstance(asset_class, list) or not all(isinstance(x, str) for x in asset_class):
        return None
    reason = obj.get("reason", "")
    if not isinstance(reason, str):
        return None
    original_cn = obj.get("original_cn", "")
    if not isinstance(original_cn, str):
        return None
    return {
        "impact": impact,
        "importance": importance,
        "asset_class": asset_class,
        "reason": reason.strip(),
        "original_cn": original_cn.strip()
    }

def iter_possible_json_blobs(text: str) -> List[str]:
    candidates: List[str] = []

    fence_patterns = [
        (r"```json\s*(.*?)\s*```", re.DOTALL),
        (r"```\s*(\{[\s\S]*?\})\s*```", re.DOTALL)
    ]
    for pat, flag in fence_patterns:
        for m in re.finditer(pat, text, flags=flag):
            candidates.append(m.group(1))

    s = text
    opens = [i for i, ch in enumerate(s) if ch == '{']
    if len(opens) <= 300:
        for i in opens:
            depth = 0
            for j in range(i, len(s)):
                if s[j] == '{':
                    depth += 1
                elif s[j] == '}':
                    depth -= 1
                    if depth == 0:
                        if j + 1 - i <= 10000:
                            candidates.append(s[i:j+1])
                        break

    m = re.search(r"\{[\s\S]*\}", text)
    if m and len(m.group(0)) <= 10000:
        candidates.append(m.group(0))

    seen = set()
    uniq: List[str] = []
    for c in candidates:
        key = hashlib.sha256(c.encode('utf-8', errors='ignore')).hexdigest()
        if key not in seen:
            seen.add(key)
            uniq.append(c)
    return uniq

def robust_extract_json(text: str) -> Optional[Dict[str, Any]]:
    try:
        obj = json.loads(text)
        valid = validate_result(obj)
        if valid:
            return valid
    except Exception:
        pass

    for cand in iter_possible_json_blobs(text):
        try:
            obj = json.loads(cand)
            valid = validate_result(obj)
            if valid:
                return valid
        except Exception:
            continue
    return None

# =========================
# RSS æ¡ç›®æ—¶é—´è§£æ
# =========================
def parse_datetime_any(dt_str: str) -> Optional[datetime.datetime]:
    if not dt_str:
        return None
    try:
        dt = email.utils.parsedate_to_datetime(dt_str)
        if dt and dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt
    except Exception:
        pass
    try:
        s = dt_str.strip().replace("Z", "+00:00")
        dt = datetime.datetime.fromisoformat(s)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt
    except Exception:
        return None

def get_entry_datetime(entry) -> Optional[datetime.datetime]:
    for k in ("published_parsed", "updated_parsed", "created_parsed"):
        if hasattr(entry, k):
            st = getattr(entry, k)
            if st:
                try:
                    ts = calendar.timegm(st)
                    return datetime.datetime.fromtimestamp(ts, tz=datetime.timezone.utc)
                except Exception:
                    pass
    for k in ("published", "pubDate", "updated", "created", "dc_date"):
        if hasattr(entry, k):
            dt = parse_datetime_any(getattr(entry, k))
            if dt:
                return dt.astimezone(datetime.timezone.utc)
    return None

def parse_rfc822_dt(dt_str: str) -> Optional[datetime.datetime]:
    try:
        return datetime.datetime.strptime(dt_str, "%a, %d %b %Y %H:%M:%S %z")
    except Exception:
        try:
            return email.utils.parsedate_to_datetime(dt_str)
        except Exception:
            return None

# =========================
# è½»é‡ POST é‡è¯•åŒ…è£…
# =========================
@dataclass
class RetryConfig:
    retries: int = 2
    backoff: float = 0.8  # seconds

def post_with_retry(session: requests.Session, url: str, *, headers=None, params=None, json_body=None,
                    timeout: float = 30.0, retry: RetryConfig = RetryConfig()):
    last_exc = None
    for i in range(retry.retries + 1):
        try:
            resp = session.post(url, headers=headers, params=params, json=json_body, timeout=timeout)
            if resp.status_code >= 500 or resp.status_code == 429:
                raise requests.HTTPError(f"status={resp.status_code}, body={resp.text[:200]}")
            return resp
        except Exception as e:
            last_exc = e
            if i < retry.retries:
                time.sleep(retry.backoff * (i + 1))
            else:
                break
    raise last_exc

# =========================
# ä¸»ç›‘æ§å™¨
# =========================
class TrumpTweetMonitor:
    def __init__(self, config_path="config.ini"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        self.config = configparser.ConfigParser()
        with open(config_path, "r", encoding="utf-8") as f:
            self.config.read_file(f)

        self.log_file = self.config.get("settings", "log_file", fallback="tweet_monitor.log")
        self.log_level = self.config.get("settings", "log_level", fallback="INFO")
        init_logging(self.log_file, self.log_level)

        self.rss_url = self.config.get("feed", "rss_url")
        self.model_provider = self.config.get("model", "provider", fallback="openai").lower()
        self.model_name = self.config.get("model", "model_name", fallback="gpt-4o")
        self.openai_key = self.config.get("openai", "api_key", fallback=None)
        self.openai_api_base = self.config.get("openai", "api_base", fallback="https://api.openai.com/v1")
        self.gemini_key = self.config.get("gemini", "api_key", fallback=None)

        self.check_interval = self.config.getint("settings", "check_interval", fallback=60)
        self.proxy_enable = self.config.getboolean("settings", "proxy_enable", fallback=True)
        self.proxy_http = self.config.get("settings", "proxy_http", fallback="http://127.0.0.1:7890")
        self.proxy_https = self.config.get("settings", "proxy_https", fallback="http://127.0.0.1:7890")
        self.analysis_record_file = self.config.get("settings", "analysis_record_file", fallback="analysis_records.json")
        self.analyzed_guids_file = self.config.get("settings", "analyzed_guids_file", fallback="analyzed_guids.json")
        self.analyzed_guids = self.load_analyzed_guids()

        # ---- æ—¶åŒº & start_time æ­£ç¡®å¤„ç†ï¼ˆé»˜è®¤ Asia/Singaporeï¼‰----
        self.timezone_name = self.config.get("settings", "timezone", fallback="Asia/Singapore")
        try:
            self.tz = ZoneInfo(self.timezone_name)
        except Exception:
            logging.warning(f"æœªçŸ¥æ—¶åŒº {self.timezone_name}ï¼Œå›é€€ä½¿ç”¨ UTC")
            self.tz = datetime.timezone.utc

        start_time_str = self.config.get("settings", "start_time", fallback=None)
        if start_time_str:
            try:
                naive = datetime.datetime.strptime(start_time_str, "%Y-%m-%d %H:%M")
                localized = naive.replace(tzinfo=self.tz)
                self.start_time_utc = localized.astimezone(datetime.timezone.utc)
            except Exception as e:
                logging.error(f"è§£æ start_time å¤±è´¥: {e}")
                self.start_time_utc = None
        else:
            self.start_time_utc = None

        self.daily_report_time = self.config.get("settings", "daily_report_time", fallback="08:00")
        self.daily_state_file = self.config.get("settings", "daily_state_file", fallback="daily_report_state.json")

        self.session = requests.Session()
        if self.proxy_enable:
            self.session.proxies.update({"http": self.proxy_http, "https": self.proxy_https})

        self.last_entry_id = None
        self.push_method = self.config.get("server", "push_method", fallback="serverchan_sdk")
        self.send_key = self.config.get("server", "send_key", fallback=None)

        self.prompt_analysis_template = _cfg_get_multiline(self.config, "prompt.analysis", "template")
        self.prompt_analysis_system_role = _cfg_get_multiline(self.config, "prompt.analysis", "system_role")
        self.prompt_analysis_json_schema = _cfg_get_multiline(self.config, "prompt.analysis", "json_schema")
        self.prompt_analysis_rules = _cfg_get_multiline(self.config, "prompt.analysis", "rules")

        self.prompt_translate_template = _cfg_get_multiline(self.config, "prompt.translate", "template")
        self.prompt_summary_template = _cfg_get_multiline(self.config, "prompt.summary", "template")

        self.push_on_impact = self.config.getboolean("push.policy", "push_on_impact", fallback=True)
        self.push_min_importance = self.config.get("push.policy", "push_min_importance", fallback="high").strip().lower()
        self.push_title = self.config.get("push.format", "title", fallback="ç‰¹æœ—æ™®æ¨æ–‡é¢„è­¦")
        self.push_tags = self.config.get("push.format", "tags", fallback="æœåŠ¡å™¨æŠ¥è­¦|å›¾ç‰‡")
        self.push_alert_template = _cfg_get_multiline(
            self.config, "push.format", "alert_template",
            "ğŸš¨ é«˜é‡è¦æ€§æ¨æ–‡ï¼š{original_cn}\nå½±å“èµ„äº§ï¼š{asset_str}\nç†ç”±ï¼š{reason}\n{link_line}"
        )
        self.push_fallback_template = _cfg_get_multiline(
            self.config, "push.format", "fallback_template",
            "âš ï¸ æœªèƒ½è§£æç»“æ„åŒ–ç»“æœï¼Œä¾›äººå·¥å¤æ ¸ï¼š\næ‘˜è¦/è¯‘æ–‡ï¼š{brief_cn}\n{link_line}"
        )
        self.push_max_title_len = self.config.getint("push.format", "max_title_len", fallback=64)
        self.push_max_desp_len = self.config.getint("push.format", "max_desp_len", fallback=512)

        self.serverchan_uid = self.config.get("serverchan", "uid", fallback="").strip()

        self.model_rpm = self.config.getint("rate_limit", "model_rpm", fallback=30)
        self.push_rpm = self.config.getint("rate_limit", "push_rpm", fallback=20)
        self.max_entries_per_cycle = self.config.getint("rate_limit", "max_entries_per_cycle", fallback=50)
        self.min_sleep_between_entries_ms = self.config.getint("rate_limit", "min_sleep_between_entries_ms", fallback=200)

        now_mono = time.monotonic()
        self._rate_limits = {
            "model": {
                "capacity": self.model_rpm,
                "tokens": float(self.model_rpm),
                "rate_per_sec": self.model_rpm / 60.0,
                "updated": now_mono,
            },
            "push": {
                "capacity": self.push_rpm,
                "tokens": float(self.push_rpm),
                "rate_per_sec": self.push_rpm / 60.0,
                "updated": now_mono,
            },
        }

        self.pusher = PusherFactory.create_pusher(
            self.push_method, self.send_key, self.session,
            self.push_title, self.push_tags, self.push_max_title_len, self.push_max_desp_len,
            self.serverchan_uid or None
        )
        if self.pusher is None:
            raise RuntimeError("æ¨é€æ–¹å¼åˆå§‹åŒ–å¤±è´¥")

        logging.info("âœ… åˆå§‹åŒ–å®Œæˆ")

    # ---------- Prompt ----------
    def build_prompt(self, tweet_text: str) -> str:
        if self.prompt_analysis_template:
            return self.prompt_analysis_template.format(
                system_role=self.prompt_analysis_system_role,
                json_schema=self.prompt_analysis_json_schema,
                rules=self.prompt_analysis_rules,
                tweet_text=tweet_text
            ).strip()
        default_template = f"""{self.prompt_analysis_system_role or "ä½ æ˜¯ä¸€åèµ„æ·±å…¨çƒå®è§‚ä¸å¤šèµ„äº§ç­–ç•¥å¸ˆã€‚è¯·å¯¹ä¸‹é¢çš„ç‰¹æœ—æ™®ç›¸å…³æ–‡æœ¬è¿›è¡Œ**å¸‚åœºå½±å“**è¯„ä¼°ã€‚è¯·æ³¨æ„ç°åœ¨çš„æ—¶é—´èŠ‚ç‚¹ç‰¹æœ—æ™®å·²ç»ä¸Šå°ä¸”ä¸¤é™¢éƒ½åœ¨å…±å’Œå…šæ‰‹ä¸­ã€‚"}

åŠ¡å¿…ä¸¥æ ¼æŒ‰ä»¥ä¸‹**å”¯ä¸€ JSON**ç»“æ„è¾“å‡ºï¼ˆä¸è¦è§£é‡Šã€ä¸è¦ä»£ç å—ã€ä¸è¦å¤šä½™æ–‡å­—ï¼‰ï¼š

{self.prompt_analysis_json_schema or '{ "impact": true|false, "importance": "high"|"medium"|"low"|"none", "asset_class": ["US equities","Treasuries","USD","Gold","Oil","China A-shares","EM FX"], "reason": "ç”¨ä¸­æ–‡ç®€æ˜è§£é‡Šä¸ºä»€ä¹ˆï¼ˆæœ€å¤š200å­—ï¼‰", "original_cn": "å¯¹åŸæ–‡çš„å‡†ç¡®ä¸­æ–‡ç¿»è¯‘æˆ–æç‚¼ï¼ˆæœ€å¤š200å­—ï¼‰" }'}

**è¯„ä¼°è¦ç‚¹ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š**
{self.prompt_analysis_rules or ""}

éœ€è¦è¯„ä¼°çš„åŸæ–‡å¦‚ä¸‹ï¼š
{tweet_text}"""
        return default_template.strip()

    # ---------- ä»¤ç‰Œæ¡¶é™é€Ÿ ----------
    def _acquire_token(self, bucket: str, permits: int = 1, timeout_sec: float = 10.0) -> bool:
        if bucket not in self._rate_limits:
            return True
        rl = self._rate_limits[bucket]
        deadline = time.monotonic() + timeout_sec
        while True:
            now = time.monotonic()
            elapsed = max(0.0, now - rl["updated"])
            refill = elapsed * rl["rate_per_sec"]
            if refill > 0:
                rl["tokens"] = min(rl["capacity"], rl["tokens"] + refill)
                rl["updated"] = now
            if rl["tokens"] >= permits:
                rl["tokens"] -= permits
                return True
            need = permits - rl["tokens"]
            wait_sec = need / rl["rate_per_sec"] if rl["rate_per_sec"] > 0 else 0.5
            if wait_sec <= 0:
                wait_sec = 0.05
            if time.monotonic() + wait_sec > deadline:
                return False
            time.sleep(min(wait_sec, 1.0))

    def push_with_rate_limit(self, message: str):
        if not self._acquire_token("push", permits=1, timeout_sec=15.0):
            logging.warning("æ¨é€é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œæœ¬æ¬¡è·³è¿‡")
            return
        self.pusher.push(message, title=self.push_title, tags=self.push_tags)

    # ---------- ç¨³å¥æ¨¡å‹è°ƒç”¨ ----------
    def analyze_with_model(self, tweet_text):
        prompt = self.build_prompt(tweet_text)
        if self.model_provider == "openai":
            return self.analyze_with_openai(prompt)
        elif self.model_provider == "gemini":
            return self.analyze_with_gemini(prompt)
        else:
            logging.error(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {self.model_provider}")
            return None

    def analyze_with_openai(self, prompt):
        try:
            if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œæœ¬æ¬¡è·³è¿‡ openai è°ƒç”¨")
                return None
            url = f"{self.openai_api_base}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_key}",
                "Content-Type": "application/json"
            }
            data = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}]
            }
            response = post_with_retry(self.session, url, headers=headers, json_body=data, timeout=30.0)
            response.raise_for_status()
            content = response.json()["choices"][0]["message"]["content"]
            return robust_extract_json(content)
        except Exception as e:
            logging.error(f"å¤§æ¨¡å‹ å¤„ç†å‡ºé”™: {e}")
            return None

    def analyze_with_gemini(self, prompt):
        try:
            if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œæœ¬æ¬¡è·³è¿‡ gemini è°ƒç”¨")
                return None
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
            headers = {"Content-Type": "application/json"}
            params = {"key": self.gemini_key}
            data = {"contents": [{"parts": [{"text": prompt}]}]}
            response = post_with_retry(self.session, url, headers=headers, params=params, json_body=data, timeout=30.0)
            response.raise_for_status()
            content = response.json()["candidates"][0]["content"]["parts"][0]["text"]
            return robust_extract_json(content)
        except Exception as e:
            logging.error(f"Gemini å¤„ç†å‡ºé”™: {e}")
            return None

    # ---------- å…œåº•åŠŸèƒ½ ----------
    def quick_translate_cn(self, text: str) -> Optional[str]:
        tmpl = self.prompt_translate_template or "è¯·æŠŠä»¥ä¸‹è‹±æ–‡æˆ–æ··åˆæ–‡æœ¬å‡†ç¡®ç¿»è¯‘æˆç®€æ´ä¸­æ–‡ï¼ˆæœ€å¤š120å­—ï¼‰ï¼Œåªè¾“å‡ºè¯‘æ–‡ï¼š\n{source_text}"
        prompt = tmpl.format(source_text=text)
        try:
            if self.model_provider == "openai":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_translate_cn")
                    return None
                url = f"{self.openai_api_base}/chat/completions"
                headers = {
                    "Authorization": f"Bearer {self.openai_key}",
                    "Content-Type": "application/json"
                }
                data = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}]}
                resp = post_with_retry(self.session, url, headers=headers, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"].strip()
            elif self.model_provider == "gemini":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_translate_cn")
                    return None
                url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_key}
                data = {"contents": [{"parts": [{"text": prompt}]}]}
                resp = post_with_retry(self.session, url, headers=headers, params=params, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
        except Exception as e:
            logging.error(f"å¿«é€Ÿç¿»è¯‘å¤±è´¥: {e}")
        return None

    def quick_summarize(self, text: str) -> Optional[str]:
        tmpl = self.prompt_summary_template or "è¯·å°†ä»¥ä¸‹å†…å®¹æ€»ç»“ä¸ºä¸€ä»½ç®€æ´çš„ä¸­æ–‡æ—¥æŠ¥ï¼ˆ200å­—å†…ï¼Œçªå‡ºé«˜é‡è¦æ€§ä¸å¸‚åœºå½±å“ï¼‰ï¼š\n{source_text}"
        prompt = tmpl.format(source_text=text)
        try:
            if self.model_provider == "openai":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_summarize")
                    return None
                url = f"{self.openai_api_base}/chat/completions"
                headers = {
                    "Authorization": f"Bearer {self.openai_key}",
                    "Content-Type": "application/json"
                }
                data = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}]}
                resp = post_with_retry(self.session, url, headers=headers, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"].strip()
            elif self.model_provider == "gemini":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_summarize")
                    return None
                url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_key}
                data = {"contents": [{"parts": [{"text": prompt}]}]}
                resp = post_with_retry(self.session, url, headers=headers, params=params, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
        except Exception as e:
            logging.error(f"æ—¥æŠ¥æ‘˜è¦å¤±è´¥: {e}")
        return None

    # ---------- è®°å½•/è¯»å†™ ----------
    def save_analysis_record(self, tweet_text, result):
        record = {
            "timestamp": datetime.datetime.now(self.tz).isoformat(),
            "tweet_text": tweet_text,
            "result": result
        }
        try:
            records = []
            if os.path.exists(self.analysis_record_file):
                with open(self.analysis_record_file, "r", encoding="utf-8") as f:
                    try:
                        records = json.load(f)
                    except Exception:
                        records = []
            records.append(record)
            with open(self.analysis_record_file, "w", encoding="utf-8") as f:
                json.dump(records, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜åˆ†æè®°å½•å¤±è´¥: {e}")

    def summarize_daily_report(self):
        today_local = datetime.datetime.now(self.tz).date()
        yesterday_local = today_local - datetime.timedelta(days=1)
        summary_records = []
        try:
            if not os.path.exists(self.analysis_record_file):
                return None
            with open(self.analysis_record_file, "r", encoding="utf-8") as f:
                records = json.load(f)
            for rec in records:
                ts = datetime.datetime.fromisoformat(rec["timestamp"])
                if ts.tzinfo is None:
                    ts = ts.replace(tzinfo=self.tz)
                ts_local = ts.astimezone(self.tz)
                if ts_local.date() == yesterday_local:
                    summary_records.append(rec)
        except Exception as e:
            logging.error(f"è¯»å–åˆ†æè®°å½•å¤±è´¥: {e}")
            return None
        if not summary_records:
            return None
        summary_text = "æ˜¨æ—¥ç‰¹æœ—æ™®æ¨æ–‡å¸‚åœºå½±å“æ—¥æŠ¥ï¼š\n"
        for rec in summary_records:
            result = rec["result"]
            if not result:
                raw = (rec.get('tweet_text', '') or '').replace("\n", " ")
                summary_text += f"- ï¼ˆæœªè§£æï¼‰{raw[:80]}...\n"
                continue
            summary_text += (
                f"- {result.get('original_cn','')} | å½±å“: {result.get('impact')} | "
                f"é‡è¦æ€§: {result.get('importance')} | èµ„äº§: {result.get('asset_class')} | "
                f"ç†ç”±: {result.get('reason')}\n"
            )
        summary = self.quick_summarize(summary_text)
        return summary or summary_text

    def send_daily_report(self):
        summary = self.summarize_daily_report()
        if summary:
            self.push_with_rate_limit(f"ã€ç‰¹æœ—æ™®æ¨æ–‡å¸‚åœºå½±å“æ—¥æŠ¥ã€‘\n{summary}")
        else:
            logging.info("æ— æ˜¨æ—¥åˆ†æè®°å½•ï¼Œæ— éœ€æ¨é€æ—¥æŠ¥ã€‚")

    def load_analyzed_guids(self):
        if os.path.exists(self.analyzed_guids_file):
            try:
                with open(self.analyzed_guids_file, "r", encoding="utf-8") as f:
                    return set(json.load(f))
            except Exception:
                return set()
        return set()

    def save_analyzed_guids(self):
        try:
            with open(self.analyzed_guids_file, "w", encoding="utf-8") as f:
                json.dump(list(self.analyzed_guids), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜å·²åˆ†ææŒ‡çº¹å¤±è´¥: {e}")

    def _load_daily_state(self) -> Dict[str, Any]:
        if os.path.exists(self.daily_state_file):
            try:
                with open(self.daily_state_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
        return {"last_sent_date": ""}

    def _save_daily_state(self, state: Dict[str, Any]):
        try:
            with open(self.daily_state_file, "w", encoding="utf-8") as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜æ—¥æŠ¥çŠ¶æ€å¤±è´¥: {e}")

    # ---------- ä¸»å¾ªç¯ ----------
    def run(self):
        while True:
            try:
                logging.info("ğŸš€ è¿›å…¥ä¸»å¾ªç¯")

                # ---- æ—¥æŠ¥è§¦å‘ï¼ˆçª—å£åˆ¤å®šï¼‰----
                try:
                    state = self._load_daily_state()
                    now_local = datetime.datetime.now(self.tz)
                    today_key = now_local.strftime("%Y-%m-%d")
                    try:
                        hh, mm = map(int, (self.daily_report_time or "08:00").split(":"))
                        target = now_local.replace(hour=hh, minute=mm, second=0, microsecond=0)
                    except Exception:
                        target = now_local.replace(hour=8, minute=0, second=0, microsecond=0)
                    window_sec = 60
                    diff = (now_local - target).total_seconds()
                    logging.debug(f"â° æ—¥æŠ¥æ£€æŸ¥ï¼šnow={now_local}, target={target}, diff={diff:.1f}s, last_sent={state.get('last_sent_date')}")
                    if state.get("last_sent_date") != today_key and 0 <= diff < window_sec:
                        logging.info("ğŸ“¨ è§¦å‘æ—¥æŠ¥å‘é€ï¼ˆçª—å£å‘½ä¸­ï¼‰")
                        self.send_daily_report()
                        state["last_sent_date"] = today_key
                        self._save_daily_state(state)
                except Exception as e:
                    logging.error(f"æ—¥æŠ¥è§¦å‘é”™è¯¯: {e}")

                # ---- æ‹‰å–RSS ----
                logging.info("ğŸ”„ å¼€å§‹ä¸€æ¬¡ RSS è½®è¯¢")
                try:
                    resp = self.session.get(self.rss_url, timeout=15)
                    resp.raise_for_status()
                    feed = feedparser.parse(resp.content)
                except Exception as e:
                    logging.error(f"RSS è·å–å¤±è´¥: {e}")
                    time.sleep(self.check_interval)
                    continue

                count = len(getattr(feed, "entries", []) or [])
                logging.info(f"ğŸ“¥ æœ¬æ¬¡è½®è¯¢è·å–åˆ°æ¡ç›®æ•°: {count}")
                if not count:
                    logging.warning("âš ï¸ æ²¡æœ‰è·å–åˆ°æ¨æ–‡")
                    time.sleep(self.check_interval)
                    continue

                entries = list(feed.entries)[: self.max_entries_per_cycle]
                logging.info(f"ğŸ§® æœ¬è½®è®¡åˆ’å¤„ç†æ¡ç›®æ•°: {len(entries)}ï¼ˆä¸Šé™ {self.max_entries_per_cycle}ï¼‰")
                for entry in entries:
                    pub_dt_utc = get_entry_datetime(entry)
                    if pub_dt_utc:
                        logging.debug(f"æ¡ç›®æ—¶é—´: {pub_dt_utc.isoformat()} vs èµ·å§‹é˜ˆå€¼: {self.start_time_utc}")
                    else:
                        logging.debug("æ¡ç›®æ—¶é—´: <æœªè§£æ> vs èµ·å§‹é˜ˆå€¼: %s", self.start_time_utc)

                    if not pub_dt_utc:
                        logging.debug("â­ï¸ è·³è¿‡ï¼šæ— æ³•è§£æå‘å¸ƒæ—¶é—´ï¼ˆæ—  *_parsed / æ— æ³•è§£æå­—ç¬¦ä¸²ï¼‰")
                        continue

                    if self.start_time_utc and pub_dt_utc < self.start_time_utc:
                        logging.debug(f"â­ï¸ è·³è¿‡ï¼šæ—©äº start_timeï¼ˆ{pub_dt_utc.isoformat()} < {self.start_time_utc.isoformat()}ï¼‰")
                        continue

                    # â€”â€” ä½¿ç”¨â€œç¨³å®šâ€æŒ‡çº¹å»é‡ï¼ˆå« URL/Unicode æ ‡å‡†åŒ–ï¼‰â€”â€”
                    fp = stable_fingerprint(entry)
                    if fp in self.analyzed_guids:
                        logging.debug("â­ï¸ è·³è¿‡ï¼šæŒ‡çº¹å·²å­˜åœ¨ï¼ˆå»é‡å‘½ä¸­ï¼‰")
                        continue

                    # â€”â€” æ„é€ å‡€æ–‡æœ¬å¹¶è¿‡æ»¤ç©ºå†…å®¹ â€”â€” 
                    tweet_text = entry_text_for_llm(entry)
                    if is_effectively_empty(tweet_text):
                        logging.debug("â­ï¸ è·³è¿‡ï¼šå‡€æ–‡æœ¬ä¸ºç©ºï¼ˆæ ‡é¢˜/æ­£æ–‡å‡æ— æœ‰æ•ˆå†…å®¹ï¼‰")
                        # å³ä¾¿ç©ºï¼Œä¹Ÿè®° fingerprintï¼Œé˜²æ­¢æºé‡å¤æ¨ç©ºå¸–åå¤å¤„ç†
                        self.analyzed_guids.add(fp)
                        self.save_analyzed_guids()
                        continue

                    link = _normalize_link(getattr(entry, "link", None))
                    logging.info(f"ğŸ†• æ£€æµ‹åˆ°æ–°æ¨æ–‡: {(_strip_html_keep_text(getattr(entry, 'title', '') or '') or '[æ— æ ‡é¢˜]')}")
                    result = self.analyze_with_model(tweet_text)

                    if result:
                        logging.info(f"âœ… åˆ†æç»“æœ: {result}")
                        self.save_analysis_record(tweet_text, result)

                        imp = str(result.get("importance", "none")).lower().strip()
                        impact_flag = bool(result.get("impact") is True)
                        threshold = IMPORTANCE_ORDER.get(self.push_min_importance, 3)
                        rank = IMPORTANCE_ORDER.get(imp, 0)
                        should_push = (not self.push_on_impact or impact_flag) and (rank >= threshold)

                        if should_push:
                            asset = result.get("asset_class")
                            asset_str = ", ".join(asset) if isinstance(asset, list) else str(asset or "")
                            link_line = f"åŸæ–‡é“¾æ¥ï¼š{link}" if link else ""
                            msg = self.push_alert_template.format(
                                original_cn=result.get("original_cn", ""),
                                asset_str=asset_str,
                                reason=result.get("reason", ""),
                                link_line=link_line
                            )
                            self.push_with_rate_limit(msg)
                    else:
                        logging.warning("âš ï¸ æ¨¡å‹æœªè¿”å›æœ‰æ•ˆ JSONï¼Œæ‰§è¡Œå›é€€æ¨é€")
                        brief_cn = self.quick_translate_cn(tweet_text) or "(ç¿»è¯‘å¤±è´¥)"
                        link_line = f"åŸæ–‡é“¾æ¥ï¼š{link}" if link else ""
                        fallback_msg = self.push_fallback_template.format(brief_cn=brief_cn, link_line=link_line)
                        self.push_with_rate_limit(fallback_msg)
                        self.save_analysis_record(tweet_text, None)

                    self.analyzed_guids.add(fp)
                    self.save_analyzed_guids()

                    if self.min_sleep_between_entries_ms > 0:
                        time.sleep(self.min_sleep_between_entries_ms / 1000.0)

                logging.info("âœ… æœ¬è½® RSS å¤„ç†ç»“æŸï¼Œè¿›å…¥ä¼‘çœ ")
                time.sleep(self.check_interval)
            except Exception as e:
                logging.error(f"è¿è¡Œæ—¶é”™è¯¯: {e}")
                time.sleep(self.check_interval)

# =========================
# å…¥å£
# =========================
if __name__ == '__main__':
    monitor = TrumpTweetMonitor()
    monitor.run()
