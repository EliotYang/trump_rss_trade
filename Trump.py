# -*- coding: utf-8 -*-
import configparser
import json
import logging
import time
import requests
import feedparser
import os
import re
import datetime
import hashlib
from typing import Any, Dict, Optional, List
from zoneinfo import ZoneInfo  # Python 3.9+
import email.utils
import calendar
from dataclasses import dataclass

# =========================
# æ—¥å¿—åˆå§‹åŒ–ï¼ˆé¿å…é‡å¤ handlersï¼‰
# =========================
def init_logging(log_file: str, log_level: str = "INFO"):
    level = getattr(logging, log_level.upper(), logging.INFO)
    root = logging.getLogger()
    if root.handlers:
        # å¦‚æœå·²ç»åˆå§‹åŒ–è¿‡ï¼Œåˆ™é‡è®¾çº§åˆ«å¹¶è¿”å›
        root.setLevel(level)
        return
    logging.basicConfig(
        level=level,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler()
        ]
    )

# =========================
# å·¥å…·ï¼šè¯»å–å¤šè¡Œå­—æ®µ
# =========================
def _cfg_get_multiline(cfg: configparser.ConfigParser, section: str, option: str, fallback: str = "") -> str:
    if cfg.has_option(section, option):
        return cfg.get(section, option).strip()
    return fallback.strip()

# =========================
# æ¨é€æŠ½è±¡ä¸å®ç°
# =========================
class Pusher:
    """æ¨é€æŠ½è±¡åŸºç±»ï¼Œä¾¿äºæ‰©å±•å¤šç§æ¨é€æ–¹å¼"""
    def push(self, message: str, title: Optional[str] = None, tags: Optional[str] = None):
        raise NotImplementedError

class ServerChanSDKPusher(Pusher):
    def __init__(self, send_key: str, session: requests.Session,
                 default_title: str = "ç‰¹æœ—æ™®æ¨æ–‡é¢„è­¦",
                 default_tags: str = "æœåŠ¡å™¨æŠ¥è­¦|å›¾ç‰‡",
                 max_title_len: int = 64,
                 max_desp_len: int = 512,
                 uid_override: Optional[str] = None):
        self.send_key = send_key
        self.session = session
        self.default_title = default_title
        self.default_tags = default_tags
        self.MAX_TITLE_LEN = max_title_len
        self.MAX_DESP_LEN = max_desp_len

        if uid_override:
            self.uid = uid_override
        else:
            # æ”¾å®½åŒ¹é…ï¼Œå°½é‡å…¼å®¹
            m = re.search(r"sctp(\d+)t", send_key or "")
            self.uid = m.group(1) if m else None
            if not self.uid:
                logging.error("send_key æ ¼å¼ä¸åŒ¹é…ä¸”æœªæä¾› serverchan.uidï¼Œæ— æ³•æ¨é€")

    def push(self, message: str, title: Optional[str] = None, tags: Optional[str] = None):
        if not self.uid:
            logging.error("æœªèƒ½æå– uid æ— æ³•æ¨é€")
            return
        safe_title = (title or self.default_title)[: self.MAX_TITLE_LEN]
        safe_desp = (message or "")[: self.MAX_DESP_LEN]
        safe_tags = tags or self.default_tags

        url = f"https://{self.uid}.push.ft07.com/send/{self.send_key}.send"
        payload = {"title": safe_title, "desp": safe_desp, "tags": safe_tags}
        headers = {"Content-Type": "application/json"}
        try:
            resp = self.session.post(url, headers=headers, json=payload, timeout=10)
            # æ‰“å°åŸå§‹è¿”å›ä»¥ä¾¿æ’é”™
            print(resp.text)
            if resp.status_code != 200:
                logging.error(f"ServerChan API æ¨é€å¤±è´¥: {resp.text}")
        except Exception as e:
            logging.error(f"ServerChan API æ¨é€å¤±è´¥: {e}")

class PusherFactory:
    @staticmethod
    def create_pusher(method: str, send_key: str, session: requests.Session,
                      title: str, tags: str, max_title_len: int, max_desp_len: int, uid_override: Optional[str]):
        if method == "serverchan_sdk" and send_key:
            return ServerChanSDKPusher(send_key, session, title, tags, max_title_len, max_desp_len, uid_override)
        else:
            logging.error("å½“å‰ä»…æ”¯æŒ serverchan_sdk æ¨é€æ–¹å¼")
            return None

# =========================
# ç¨³å¥ JSON è§£æä¸æ ¡éªŒ
# =========================
ALLOWED_IMPORTANCE = {"high", "medium", "low", "none"}
IMPORTANCE_ORDER = {"none": 0, "low": 1, "medium": 2, "high": 3}

def validate_result(obj: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    æœ€å°å¯ç”¨çš„schemaæ ¡éªŒ & è§„èŒƒåŒ–ï¼š
    - impact: bool
    - importance: in {"high","medium","low","none"}
    - asset_class: list[str]
    - reason: str
    - original_cn: str
    """
    if not isinstance(obj, dict):
        return None
    impact = obj.get("impact", None)
    if not isinstance(impact, bool):
        return None
    importance = str(obj.get("importance", "none")).lower().strip()
    if importance not in ALLOWED_IMPORTANCE:
        return None
    asset_class = obj.get("asset_class", [])
    if asset_class is None:
        asset_class = []
    if not isinstance(asset_class, list) or not all(isinstance(x, str) for x in asset_class):
        return None
    reason = obj.get("reason", "")
    if not isinstance(reason, str):
        return None
    original_cn = obj.get("original_cn", "")
    if not isinstance(original_cn, str):
        return None
    # è§„èŒƒåŒ–è¿”å›
    return {
        "impact": impact,
        "importance": importance,
        "asset_class": asset_class,
        "reason": reason.strip(),
        "original_cn": original_cn.strip()
    }

def iter_possible_json_blobs(text: str) -> List[str]:
    """
    è¿”å›æ–‡æœ¬ä¸­å¯èƒ½çš„ JSON ç‰‡æ®µå€™é€‰ï¼š
    - å¤„ç† ```json ... ``` ä»£ç å—
    - å¤„ç†è£¸ JSON
    - å¤„ç†åµŒå¥—èŠ±æ‹¬å·çš„å¹³è¡¡æ‰«æ
    """
    candidates: List[str] = []

    # 1) ```json ... ``` or ``` ... ```
    fence_patterns = [
        (r"```json\s*(.*?)\s*```", re.DOTALL),
        (r"```\s*(\{[\s\S]*?\})\s*```", re.DOTALL)
    ]
    for pat, flag in fence_patterns:
        for m in re.finditer(pat, text, flags=flag):
            candidates.append(m.group(1))

    # 2) å¹³è¡¡æ‰«æï¼šå¯»æ‰¾æ¯ä¸ª '{' å‡ºå‘çš„æˆå¯¹ '}' ç‰‡æ®µï¼ˆé™åˆ¶å¤æ‚åº¦ï¼‰
    s = text
    opens = [i for i, ch in enumerate(s) if ch == '{']
    if len(opens) <= 300:  # ç²—ç•¥é™åˆ¶ï¼Œé˜²æ­¢è¶…å¤§æ–‡æœ¬è€—æ—¶
        for i in opens:
            depth = 0
            for j in range(i, len(s)):
                if s[j] == '{':
                    depth += 1
                elif s[j] == '}':
                    depth -= 1
                    if depth == 0:
                        # é™åˆ¶å•ç‰‡æ®µæœ€å¤§é•¿åº¦ï¼Œé¿å…åæ‰æ•´ç¯‡
                        if j + 1 - i <= 10000:
                            candidates.append(s[i:j+1])
                        break

    # 3) ç®€å•æ­£åˆ™çš„ç¬¬ä¸€ä¸ª {...}ï¼ˆå®¹æ˜“è´ªå©ªï¼Œæ”¾åˆ°æœ€åä½œä¸ºå…œåº•ï¼‰
    m = re.search(r"\{[\s\S]*\}", text)
    if m and len(m.group(0)) <= 10000:
        candidates.append(m.group(0))

    # å»é‡ï¼Œä¿åº
    seen = set()
    uniq: List[str] = []
    for c in candidates:
        key = hashlib.sha256(c.encode('utf-8', errors='ignore')).hexdigest()
        if key not in seen:
            seen.add(key)
            uniq.append(c)
    return uniq

def robust_extract_json(text: str) -> Optional[Dict[str, Any]]:
    """
    è¿­ä»£å°è¯•ä¸åŒå€™é€‰ç‰‡æ®µè¿›è¡Œ json.loads + schema æ ¡éªŒã€‚
    """
    # å…ˆç›´æ¥å°è¯•æ•´ä½“
    try:
        obj = json.loads(text)
        valid = validate_result(obj)
        if valid:
            return valid
    except Exception:
        pass

    # é€ä¸ªå€™é€‰
    for cand in iter_possible_json_blobs(text):
        try:
            obj = json.loads(cand)
            valid = validate_result(obj)
            if valid:
                return valid
        except Exception:
            continue
    return None

# =========================
# RSS æ¡ç›®å»é‡æŒ‡çº¹ & æ—¶é—´è§£æ
# =========================
def get_entry_fingerprint(entry) -> str:
    """
    æŒ‡çº¹ä¼˜å…ˆé¡ºåºï¼š
    1. entry.id æˆ– guid
    2. link
    3. title + pubDate
    æœ€ååš sha256
    """
    parts: List[str] = []

    # ä¼˜å…ˆ id/guid/link
    val = None
    if hasattr(entry, "id") and entry.id:
        val = str(entry.id)
    elif hasattr(entry, "guid") and entry.guid:
        val = str(entry.guid)
    elif hasattr(entry, "link") and entry.link:
        val = str(entry.link)
    else:
        # å…œåº•ï¼štitle + pubDate
        title = getattr(entry, "title", "") or ""
        pub_date_str = getattr(entry, "pubDate", None) or getattr(entry, "published", None) or ""
        if title:
            parts.append(str(title))
        if pub_date_str:
            parts.append(str(pub_date_str))

    if val:
        parts.append(val)

    if not parts:
        # æç«¯å…œåº•ï¼šå°†æ•´ä¸ª entry æ–‡æœ¬åŒ–
        parts.append(str(entry))
    raw = "||".join(parts)
    return hashlib.sha256(raw.encode("utf-8", errors="ignore")).hexdigest()

def parse_datetime_any(dt_str: str) -> Optional[datetime.datetime]:
    """
    æ—¢æ”¯æŒ RFC822/é‚®ä»¶å¤´ï¼Œä¹Ÿæ”¯æŒ ISO8601ï¼ˆå«Zï¼‰ã€‚
    è¿”å›å¸¦æ—¶åŒºçš„ aware datetimeï¼ˆå°½é‡è½¬ä¸º UTCï¼‰ã€‚
    """
    if not dt_str:
        return None
    # 1) å…ˆèµ°é‚®ä»¶å¤´é£æ ¼
    try:
        dt = email.utils.parsedate_to_datetime(dt_str)
        if dt and dt.tzinfo is None:
            # å‡å®šä¸º UTC
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt
    except Exception:
        pass
    # 2) ISO8601: å…è®¸å°¾éƒ¨ Z
    try:
        s = dt_str.strip().replace("Z", "+00:00")
        dt = datetime.datetime.fromisoformat(s)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt
    except Exception:
        return None

def get_entry_datetime(entry) -> Optional[datetime.datetime]:
    """
    ä¼˜å…ˆä½¿ç”¨ feedparser çš„ *_parsed å­—æ®µï¼›å…¶æ¬¡è§£æå¸¸è§å­—ç¬¦ä¸²å­—æ®µã€‚
    ç»Ÿä¸€è¿”å› UTC aware datetimeã€‚
    """
    # a) *_parsedï¼ˆstruct_timeï¼Œé€šå¸¸ä¸º GMT/UTCï¼‰
    for k in ("published_parsed", "updated_parsed", "created_parsed"):
        if hasattr(entry, k):
            st = getattr(entry, k)
            if st:
                try:
                    ts = calendar.timegm(st)  # æŒ‰UTCè§£é‡Š
                    return datetime.datetime.fromtimestamp(ts, tz=datetime.timezone.utc)
                except Exception:
                    pass
    # b) å¸¸è§å­—ç¬¦ä¸²å­—æ®µ
    for k in ("published", "pubDate", "updated", "created", "dc_date"):
        if hasattr(entry, k):
            dt = parse_datetime_any(getattr(entry, k))
            if dt:
                return dt.astimezone(datetime.timezone.utc)
    return None

def parse_rfc822_dt(dt_str: str) -> Optional[datetime.datetime]:
    """
    è§£æ RFC822/é€šç”¨é‚®ä»¶æ—¥æœŸä¸º aware datetime
    """
    try:
        return datetime.datetime.strptime(dt_str, "%a, %d %b %Y %H:%M:%S %z")
    except Exception:
        try:
            return email.utils.parsedate_to_datetime(dt_str)
        except Exception:
            return None

# =========================
# è½»é‡ POST é‡è¯•åŒ…è£…ï¼ˆå¯é€‰ï¼‰
# =========================
@dataclass
class RetryConfig:
    retries: int = 2
    backoff: float = 0.8  # seconds

def post_with_retry(session: requests.Session, url: str, *, headers=None, params=None, json_body=None,
                    timeout: float = 30.0, retry: RetryConfig = RetryConfig()):
    last_exc = None
    for i in range(retry.retries + 1):
        try:
            resp = session.post(url, headers=headers, params=params, json=json_body, timeout=timeout)
            # å¯¹ 429/5xx åšç®€å•é‡è¯•
            if resp.status_code >= 500 or resp.status_code == 429:
                raise requests.HTTPError(f"status={resp.status_code}, body={resp.text[:200]}")
            return resp
        except Exception as e:
            last_exc = e
            if i < retry.retries:
                time.sleep(retry.backoff * (i + 1))
            else:
                break
    raise last_exc

# =========================
# ä¸»ç›‘æ§å™¨
# =========================
class TrumpTweetMonitor:
    def __init__(self, config_path="config.ini"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        # è¯»å–é…ç½®
        self.config = configparser.ConfigParser()
        with open(config_path, "r", encoding="utf-8") as f:
            self.config.read_file(f)

        # å…ˆè¯»å–æ—¥å¿—é…ç½®å¹¶ç«‹å³åˆå§‹åŒ–æ—¥å¿—ï¼ˆç¡®ä¿æœ€æ—©çš„æ—¥å¿—ä¹Ÿèƒ½å†™å…¥ï¼‰
        self.log_file = self.config.get("settings", "log_file", fallback="tweet_monitor.log")
        self.log_level = self.config.get("settings", "log_level", fallback="INFO")
        init_logging(self.log_file, self.log_level)

        self.rss_url = self.config.get("feed", "rss_url")
        self.model_provider = self.config.get("model", "provider", fallback="openai").lower()
        self.model_name = self.config.get("model", "model_name", fallback="gpt-4o")
        self.openai_key = self.config.get("openai", "api_key", fallback=None)
        self.openai_api_base = self.config.get("openai", "api_base", fallback="https://api.openai.com/v1")
        self.gemini_key = self.config.get("gemini", "api_key", fallback=None)

        self.check_interval = self.config.getint("settings", "check_interval", fallback=60)
        self.proxy_enable = self.config.getboolean("settings", "proxy_enable", fallback=True)
        self.proxy_http = self.config.get("settings", "proxy_http", fallback="http://127.0.0.1:7890")
        self.proxy_https = self.config.get("settings", "proxy_https", fallback="http://127.0.0.1:7890")
        self.analysis_record_file = self.config.get("settings", "analysis_record_file", fallback="analysis_records.json")
        self.analyzed_guids_file = self.config.get("settings", "analyzed_guids_file", fallback="analyzed_guids.json")
        self.analyzed_guids = self.load_analyzed_guids()

        # ---- æ—¶åŒº & start_time æ­£ç¡®å¤„ç† ----
        self.timezone_name = self.config.get("settings", "timezone", fallback="Asia/Taipei")
        try:
            self.tz = ZoneInfo(self.timezone_name)
        except Exception:
            logging.warning(f"æœªçŸ¥æ—¶åŒº {self.timezone_name}ï¼Œå›é€€ä½¿ç”¨ UTC")
            self.tz = datetime.timezone.utc

        start_time_str = self.config.get("settings", "start_time", fallback=None)
        if start_time_str:
            try:
                # å‡å®šé…ç½®æ˜¯â€œæœ¬åœ°æ—¶åŒºâ€çš„äººç±»æ—¶é—´ï¼Œå…ˆæœ¬åœ°åŒ–ï¼Œå†è½¬ UTC
                naive = datetime.datetime.strptime(start_time_str, "%Y-%m-%d %H:%M")
                localized = naive.replace(tzinfo=self.tz)
                self.start_time_utc = localized.astimezone(datetime.timezone.utc)
            except Exception as e:
                logging.error(f"è§£æ start_time å¤±è´¥: {e}")
                self.start_time_utc = None
        else:
            self.start_time_utc = None

        # ---- æ—¥æŠ¥è§¦å‘ï¼šæ˜¾å¼æ—¶åˆ»ï¼ˆé»˜è®¤ 08:00ï¼‰ ----
        self.daily_report_time = self.config.get("settings", "daily_report_time", fallback="08:00")
        self.daily_state_file = self.config.get("settings", "daily_state_file", fallback="daily_report_state.json")

        # HTTP ä¼šè¯ï¼ˆå¸¦å¯é€‰ä»£ç†ï¼‰
        self.session = requests.Session()
        if self.proxy_enable:
            self.session.proxies.update({"http": self.proxy_http, "https": self.proxy_https})

        self.last_entry_id = None
        self.push_method = self.config.get("server", "push_method", fallback="serverchan_sdk")
        self.send_key = self.config.get("server", "send_key", fallback=None)

        # ---- Prompt é…ç½®è¯»å– ----
        self.prompt_analysis_template = _cfg_get_multiline(self.config, "prompt.analysis", "template")
        self.prompt_analysis_system_role = _cfg_get_multiline(self.config, "prompt.analysis", "system_role")
        self.prompt_analysis_json_schema = _cfg_get_multiline(self.config, "prompt.analysis", "json_schema")
        self.prompt_analysis_rules = _cfg_get_multiline(self.config, "prompt.analysis", "rules")

        self.prompt_translate_template = _cfg_get_multiline(self.config, "prompt.translate", "template")
        self.prompt_summary_template = _cfg_get_multiline(self.config, "prompt.summary", "template")

        # ---- æ¨é€ç­–ç•¥/æ–‡æ¡ˆ ----
        self.push_on_impact = self.config.getboolean("push.policy", "push_on_impact", fallback=True)
        self.push_min_importance = self.config.get("push.policy", "push_min_importance", fallback="high").strip().lower()
        self.push_title = self.config.get("push.format", "title", fallback="ç‰¹æœ—æ™®æ¨æ–‡é¢„è­¦")
        self.push_tags = self.config.get("push.format", "tags", fallback="æœåŠ¡å™¨æŠ¥è­¦|å›¾ç‰‡")
        self.push_alert_template = _cfg_get_multiline(
            self.config, "push.format", "alert_template",
            "ğŸš¨ é«˜é‡è¦æ€§æ¨æ–‡ï¼š{original_cn}\nå½±å“èµ„äº§ï¼š{asset_str}\nç†ç”±ï¼š{reason}\n{link_line}"
        )
        self.push_fallback_template = _cfg_get_multiline(
            self.config, "push.format", "fallback_template",
            "âš ï¸ æœªèƒ½è§£æç»“æ„åŒ–ç»“æœï¼Œä¾›äººå·¥å¤æ ¸ï¼š\næ‘˜è¦/è¯‘æ–‡ï¼š{brief_cn}\n{link_line}"
        )
        self.push_max_title_len = self.config.getint("push.format", "max_title_len", fallback=64)
        self.push_max_desp_len = self.config.getint("push.format", "max_desp_len", fallback=512)

        # ---- ServerChan uid å…œåº• ----
        self.serverchan_uid = self.config.get("serverchan", "uid", fallback="").strip()

        # ---- é™é€Ÿé…ç½®ï¼ˆToken Bucketï¼‰----
        self.model_rpm = self.config.getint("rate_limit", "model_rpm", fallback=30)
        self.push_rpm = self.config.getint("rate_limit", "push_rpm", fallback=20)
        self.max_entries_per_cycle = self.config.getint("rate_limit", "max_entries_per_cycle", fallback=50)
        self.min_sleep_between_entries_ms = self.config.getint("rate_limit", "min_sleep_between_entries_ms", fallback=200)

        now_mono = time.monotonic()
        self._rate_limits = {
            "model": {
                "capacity": self.model_rpm,
                "tokens": float(self.model_rpm),
                "rate_per_sec": self.model_rpm / 60.0,
                "updated": now_mono,
            },
            "push": {
                "capacity": self.push_rpm,
                "tokens": float(self.push_rpm),
                "rate_per_sec": self.push_rpm / 60.0,
                "updated": now_mono,
            },
        }

        # ---- æ¨é€å™¨ ----
        self.pusher = PusherFactory.create_pusher(
            self.push_method, self.send_key, self.session,
            self.push_title, self.push_tags, self.push_max_title_len, self.push_max_desp_len,
            self.serverchan_uid or None
        )
        if self.pusher is None:
            raise RuntimeError("æ¨é€æ–¹å¼åˆå§‹åŒ–å¤±è´¥")

        logging.info("âœ… åˆå§‹åŒ–å®Œæˆ")

    # ---------- Prompt ----------
    def build_prompt(self, tweet_text: str) -> str:
        # é…ç½®åŒ–æ¨¡æ¿ï¼šæ”¯æŒ {system_role} / {json_schema} / {rules} / {tweet_text}
        if self.prompt_analysis_template:
            return self.prompt_analysis_template.format(
                system_role=self.prompt_analysis_system_role,
                json_schema=self.prompt_analysis_json_schema,
                rules=self.prompt_analysis_rules,
                tweet_text=tweet_text
            ).strip()
        # å…¼å®¹æ—§ç‰ˆå›é€€
        default_template = f"""{self.prompt_analysis_system_role or "ä½ æ˜¯ä¸€åèµ„æ·±å…¨çƒå®è§‚ä¸å¤šèµ„äº§ç­–ç•¥å¸ˆã€‚è¯·å¯¹ä¸‹é¢çš„ç‰¹æœ—æ™®ç›¸å…³æ–‡æœ¬è¿›è¡Œ**å¸‚åœºå½±å“**è¯„ä¼°ã€‚è¯·æ³¨æ„ç°åœ¨çš„æ—¶é—´èŠ‚ç‚¹ç‰¹æœ—æ™®å·²ç»ä¸Šå°ä¸”ä¸¤é™¢éƒ½åœ¨å…±å’Œå…šæ‰‹ä¸­ã€‚"}

åŠ¡å¿…ä¸¥æ ¼æŒ‰ä»¥ä¸‹**å”¯ä¸€ JSON**ç»“æ„è¾“å‡ºï¼ˆä¸è¦è§£é‡Šã€ä¸è¦ä»£ç å—ã€ä¸è¦å¤šä½™æ–‡å­—ï¼‰ï¼š

{self.prompt_analysis_json_schema or '{ "impact": true|false, "importance": "high"|"medium"|"low"|"none", "asset_class": ["US equities","Treasuries","USD","Gold","Oil","China A-shares","EM FX"], "reason": "ç”¨ä¸­æ–‡ç®€æ˜è§£é‡Šä¸ºä»€ä¹ˆï¼ˆæœ€å¤š200å­—ï¼‰", "original_cn": "å¯¹åŸæ–‡çš„å‡†ç¡®ä¸­æ–‡ç¿»è¯‘æˆ–æç‚¼ï¼ˆæœ€å¤š200å­—ï¼‰" }'}

**è¯„ä¼°è¦ç‚¹ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š**
{self.prompt_analysis_rules or ""}

éœ€è¦è¯„ä¼°çš„åŸæ–‡å¦‚ä¸‹ï¼š
{tweet_text}"""
        return default_template.strip()

    # ---------- ä»¤ç‰Œæ¡¶é™é€Ÿ ----------
    def _acquire_token(self, bucket: str, permits: int = 1, timeout_sec: float = 10.0) -> bool:
        """
        ç®€å•ä»¤ç‰Œæ¡¶ï¼šä¸è¶³åˆ™ç­‰å¾…ï¼Œç›´åˆ°è¶…æ—¶æˆ–è·å¾—ã€‚
        bucket: "model" | "push"
        """
        if bucket not in self._rate_limits:
            return True  # æœªé…ç½®åˆ™ç›´æ¥æ”¾è¡Œ
        rl = self._rate_limits[bucket]
        deadline = time.monotonic() + timeout_sec
        while True:
            now = time.monotonic()
            # å…ˆè¡¥å‘ä»¤ç‰Œ
            elapsed = max(0.0, now - rl["updated"])
            refill = elapsed * rl["rate_per_sec"]
            if refill > 0:
                rl["tokens"] = min(rl["capacity"], rl["tokens"] + refill)
                rl["updated"] = now
            if rl["tokens"] >= permits:
                rl["tokens"] -= permits
                return True
            # ä¸å¤Ÿä»¤ç‰Œï¼Œè®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            need = permits - rl["tokens"]
            wait_sec = need / rl["rate_per_sec"] if rl["rate_per_sec"] > 0 else 0.5
            if wait_sec <= 0:
                wait_sec = 0.05
            if time.monotonic() + wait_sec > deadline:
                return False
            time.sleep(min(wait_sec, 1.0))

    def push_with_rate_limit(self, message: str):
        if not self._acquire_token("push", permits=1, timeout_sec=15.0):
            logging.warning("æ¨é€é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œæœ¬æ¬¡è·³è¿‡")
            return
        self.pusher.push(message, title=self.push_title, tags=self.push_tags)

    # ---------- ç¨³å¥æ¨¡å‹è°ƒç”¨ ----------
    def analyze_with_model(self, tweet_text):
        prompt = self.build_prompt(tweet_text)
        if self.model_provider == "openai":
            return self.analyze_with_openai(prompt)
        elif self.model_provider == "gemini":
            return self.analyze_with_gemini(prompt)
        else:
            logging.error(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {self.model_provider}")
            return None

    def analyze_with_openai(self, prompt):
        try:
            if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œæœ¬æ¬¡è·³è¿‡ openai è°ƒç”¨")
                return None
            url = f"{self.openai_api_base}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_key}",
                "Content-Type": "application/json"
            }
            data = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}]
            }
            response = post_with_retry(self.session, url, headers=headers, json_body=data, timeout=30.0)
            response.raise_for_status()
            content = response.json()["choices"][0]["message"]["content"]
            return robust_extract_json(content)
        except Exception as e:
            logging.error(f"å¤§æ¨¡å‹ å¤„ç†å‡ºé”™: {e}")
            return None

    def analyze_with_gemini(self, prompt):
        try:
            if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œæœ¬æ¬¡è·³è¿‡ gemini è°ƒç”¨")
                return None
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
            headers = {"Content-Type": "application/json"}
            params = {"key": self.gemini_key}
            data = {"contents": [{"parts": [{"text": prompt}]}]}
            response = post_with_retry(self.session, url, headers=headers, params=params, json_body=data, timeout=30.0)
            response.raise_for_status()
            content = response.json()["candidates"][0]["content"]["parts"][0]["text"]
            return robust_extract_json(content)
        except Exception as e:
            logging.error(f"Gemini å¤„ç†å‡ºé”™: {e}")
            return None

    # ---------- å…œåº•åŠŸèƒ½ ----------
    def quick_translate_cn(self, text: str) -> Optional[str]:
        """
        åœ¨æ¨¡å‹æ— æ³•è¿”å›JSONæ—¶ï¼Œç”¨æç®€promptåšå…œåº•ä¸­æ–‡ç¿»è¯‘ï¼ˆæœ€å¤š120å­—ï¼‰ã€‚
        æ³¨æ„ï¼šä¸è¦æ±‚JSONï¼Œè¿”å›å­—ç¬¦ä¸²å³å¯ã€‚
        """
        tmpl = self.prompt_translate_template or "è¯·æŠŠä»¥ä¸‹è‹±æ–‡æˆ–æ··åˆæ–‡æœ¬å‡†ç¡®ç¿»è¯‘æˆç®€æ´ä¸­æ–‡ï¼ˆæœ€å¤š120å­—ï¼‰ï¼Œåªè¾“å‡ºè¯‘æ–‡ï¼š\n{source_text}"
        prompt = tmpl.format(source_text=text)
        try:
            if self.model_provider == "openai":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_translate_cn")
                    return None
                url = f"{self.openai_api_base}/chat/completions"
                headers = {
                    "Authorization": f"Bearer {self.openai_key}",
                    "Content-Type": "application/json"
                }
                data = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}]}
                resp = post_with_retry(self.session, url, headers=headers, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"].strip()
            elif self.model_provider == "gemini":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_translate_cn")
                    return None
                url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_key}
                data = {"contents": [{"parts": [{"text": prompt}]}]}
                resp = post_with_retry(self.session, url, headers=headers, params=params, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
        except Exception as e:
            logging.error(f"å¿«é€Ÿç¿»è¯‘å¤±è´¥: {e}")
        return None

    def quick_summarize(self, text: str) -> Optional[str]:
        """
        æ—¥æŠ¥æ‘˜è¦ï¼šè®©æ¨¡å‹åšç®€çŸ­ä¸­æ–‡æ‘˜è¦ï¼ˆä¸è¦æ±‚ JSONï¼‰ï¼Œè¿”å›å­—ç¬¦ä¸²ã€‚
        """
        tmpl = self.prompt_summary_template or "è¯·å°†ä»¥ä¸‹å†…å®¹æ€»ç»“ä¸ºä¸€ä»½ç®€æ´çš„ä¸­æ–‡æ—¥æŠ¥ï¼ˆ200å­—å†…ï¼Œçªå‡ºé«˜é‡è¦æ€§ä¸å¸‚åœºå½±å“ï¼‰ï¼š\n{source_text}"
        prompt = tmpl.format(source_text=text)
        try:
            if self.model_provider == "openai":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_summarize")
                    return None
                url = f"{self.openai_api_base}/chat/completions"
                headers = {
                    "Authorization": f"Bearer {self.openai_key}",
                    "Content-Type": "application/json"
                }
                data = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}]}
                resp = post_with_retry(self.session, url, headers=headers, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"].strip()
            elif self.model_provider == "gemini":
                if not self._acquire_token("model", permits=1, timeout_sec=15.0):
                    logging.warning("æ¨¡å‹é™é€Ÿæœªè·å¾—ä»¤ç‰Œï¼Œè·³è¿‡ quick_summarize")
                    return None
                url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_key}
                data = {"contents": [{"parts": [{"text": prompt}]}]}
                resp = post_with_retry(self.session, url, headers=headers, params=params, json_body=data, timeout=30.0)
                resp.raise_for_status()
                return resp.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
        except Exception as e:
            logging.error(f"æ—¥æŠ¥æ‘˜è¦å¤±è´¥: {e}")
        return None

    # ---------- è®°å½•/è¯»å†™ ----------
    def save_analysis_record(self, tweet_text, result):
        record = {
            "timestamp": datetime.datetime.now(self.tz).isoformat(),
            "tweet_text": tweet_text,
            "result": result
        }
        try:
            records = []
            if os.path.exists(self.analysis_record_file):
                with open(self.analysis_record_file, "r", encoding="utf-8") as f:
                    try:
                        records = json.load(f)
                    except Exception:
                        records = []
            records.append(record)
            with open(self.analysis_record_file, "w", encoding="utf-8") as f:
                json.dump(records, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜åˆ†æè®°å½•å¤±è´¥: {e}")

    def summarize_daily_report(self):
        today_local = datetime.datetime.now(self.tz).date()
        yesterday_local = today_local - datetime.timedelta(days=1)
        summary_records = []
        try:
            if not os.path.exists(self.analysis_record_file):
                return None
            with open(self.analysis_record_file, "r", encoding="utf-8") as f:
                records = json.load(f)
            for rec in records:
                ts = datetime.datetime.fromisoformat(rec["timestamp"])
                if ts.tzinfo is None:
                    ts = ts.replace(tzinfo=self.tz)
                ts_local = ts.astimezone(self.tz)
                if ts_local.date() == yesterday_local:
                    summary_records.append(rec)
        except Exception as e:
            logging.error(f"è¯»å–åˆ†æè®°å½•å¤±è´¥: {e}")
            return None
        if not summary_records:
            return None
        # æ‹¼æ¥æ‘˜è¦åŸå§‹æ–‡æœ¬
        summary_text = "æ˜¨æ—¥ç‰¹æœ—æ™®æ¨æ–‡å¸‚åœºå½±å“æ—¥æŠ¥ï¼š\n"
        for rec in summary_records:
            result = rec["result"]
            if not result:
                # è®°å½•è¿‡ä½†æ— ç»“æ„åŒ–ç»“æœ
                raw = (rec.get('tweet_text', '') or '').replace("\n", " ")
                summary_text += f"- ï¼ˆæœªè§£æï¼‰{raw[:80]}...\n"
                continue
            summary_text += (
                f"- {result.get('original_cn','')} | å½±å“: {result.get('impact')} | "
                f"é‡è¦æ€§: {result.get('importance')} | èµ„äº§: {result.get('asset_class')} | "
                f"ç†ç”±: {result.get('reason')}\n"
            )
        # äº¤ç»™æ¨¡å‹åšç²¾ç®€ï¼ˆé JSONï¼‰
        summary = self.quick_summarize(summary_text)
        return summary or summary_text

    def send_daily_report(self):
        summary = self.summarize_daily_report()
        if summary:
            self.push_with_rate_limit(f"ã€ç‰¹æœ—æ™®æ¨æ–‡å¸‚åœºå½±å“æ—¥æŠ¥ã€‘\n{summary}")
        else:
            logging.info("æ— æ˜¨æ—¥åˆ†æè®°å½•ï¼Œæ— éœ€æ¨é€æ—¥æŠ¥ã€‚")

    def load_analyzed_guids(self):
        if os.path.exists(self.analyzed_guids_file):
            try:
                with open(self.analyzed_guids_file, "r", encoding="utf-8") as f:
                    return set(json.load(f))
            except Exception:
                return set()
        return set()

    def save_analyzed_guids(self):
        try:
            with open(self.analyzed_guids_file, "w", encoding="utf-8") as f:
                json.dump(list(self.analyzed_guids), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜å·²åˆ†ææŒ‡çº¹å¤±è´¥: {e}")

    def _load_daily_state(self) -> Dict[str, Any]:
        if os.path.exists(self.daily_state_file):
            try:
                with open(self.daily_state_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
        return {"last_sent_date": ""}

    def _save_daily_state(self, state: Dict[str, Any]):
        try:
            with open(self.daily_state_file, "w", encoding="utf-8") as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜æ—¥æŠ¥çŠ¶æ€å¤±è´¥: {e}")

    # ---------- ä¸»å¾ªç¯ ----------
    def run(self):
        while True:
            try:
                logging.info("ğŸš€ è¿›å…¥ä¸»å¾ªç¯")

                # ---- å¯é çš„æ—¥æŠ¥è§¦å‘ï¼ˆçª—å£åˆ¤å®šï¼‰----
                try:
                    state = self._load_daily_state()
                    now_local = datetime.datetime.now(self.tz)
                    today_key = now_local.strftime("%Y-%m-%d")
                    try:
                        hh, mm = map(int, (self.daily_report_time or "08:00").split(":"))
                        target = now_local.replace(hour=hh, minute=mm, second=0, microsecond=0)
                    except Exception:
                        target = now_local.replace(hour=8, minute=0, second=0, microsecond=0)
                    window_sec = 60  # 60ç§’çª—å£
                    diff = (now_local - target).total_seconds()
                    logging.debug(f"â° æ—¥æŠ¥æ£€æŸ¥ï¼šnow={now_local}, target={target}, diff={diff:.1f}s, last_sent={state.get('last_sent_date')}")
                    if state.get("last_sent_date") != today_key and 0 <= diff < window_sec:
                        logging.info("ğŸ“¨ è§¦å‘æ—¥æŠ¥å‘é€ï¼ˆçª—å£å‘½ä¸­ï¼‰")
                        self.send_daily_report()
                        state["last_sent_date"] = today_key
                        self._save_daily_state(state)
                except Exception as e:
                    logging.error(f"æ—¥æŠ¥è§¦å‘é”™è¯¯: {e}")

                # ---- æ‹‰å–RSSï¼ˆéµå¾ªä»£ç†ï¼‰----
                logging.info("ğŸ”„ å¼€å§‹ä¸€æ¬¡ RSS è½®è¯¢")
                try:
                    resp = self.session.get(self.rss_url, timeout=15)
                    resp.raise_for_status()
                    feed = feedparser.parse(resp.content)
                except Exception as e:
                    logging.error(f"RSS è·å–å¤±è´¥: {e}")
                    time.sleep(self.check_interval)
                    continue

                count = len(getattr(feed, "entries", []) or [])
                logging.info(f"ğŸ“¥ æœ¬æ¬¡è½®è¯¢è·å–åˆ°æ¡ç›®æ•°: {count}")
                if not count:
                    logging.warning("âš ï¸ æ²¡æœ‰è·å–åˆ°æ¨æ–‡")
                    time.sleep(self.check_interval)
                    continue

                # ---- é™åˆ¶æœ¬è½®æœ€å¤šå¤„ç†çš„æ¡ç›®æ•° ----
                entries = list(feed.entries)[: self.max_entries_per_cycle]
                logging.info(f"ğŸ§® æœ¬è½®è®¡åˆ’å¤„ç†æ¡ç›®æ•°: {len(entries)}ï¼ˆä¸Šé™ {self.max_entries_per_cycle}ï¼‰")
                for entry in entries:
                    # ç»Ÿä¸€è·å–å‘å¸ƒæ—¶é—´ï¼ˆUTC awareï¼‰
                    pub_dt_utc = get_entry_datetime(entry)
                    if pub_dt_utc:
                        logging.debug(f"æ¡ç›®æ—¶é—´: {pub_dt_utc.isoformat()} vs èµ·å§‹é˜ˆå€¼: {self.start_time_utc}")
                    else:
                        logging.debug("æ¡ç›®æ—¶é—´: <æœªè§£æ> vs èµ·å§‹é˜ˆå€¼: %s", self.start_time_utc)

                    if not pub_dt_utc:
                        logging.debug("â­ï¸ è·³è¿‡ï¼šæ— æ³•è§£æå‘å¸ƒæ—¶é—´ï¼ˆæ—  *_parsed / æ— æ³•è§£æå­—ç¬¦ä¸²ï¼‰")
                        continue

                    # start_time è¿‡æ»¤ï¼ˆç›´æ¥å¯¹ UTC æ¯”è¾ƒï¼‰
                    if self.start_time_utc and pub_dt_utc < self.start_time_utc:
                        logging.debug(f"â­ï¸ è·³è¿‡ï¼šæ—©äº start_timeï¼ˆ{pub_dt_utc.isoformat()} < {self.start_time_utc.isoformat()}ï¼‰")
                        continue

                    # ç¨³å®šæŒ‡çº¹å»é‡
                    fp = get_entry_fingerprint(entry)
                    if fp in self.analyzed_guids:
                        logging.debug("â­ï¸ è·³è¿‡ï¼šæŒ‡çº¹å·²å­˜åœ¨ï¼ˆå»é‡å‘½ä¸­ï¼‰")
                        continue

                    title = getattr(entry, "title", "").strip() if hasattr(entry, "title") else ""
                    description = getattr(entry, "description", "").strip() if hasattr(entry, "description") else ""
                    link = getattr(entry, "link", None)

                    if not title and not description:
                        logging.debug("â­ï¸ è·³è¿‡ï¼šæ— æ ‡é¢˜ä¸”æ— æ‘˜è¦/æè¿°")
                        continue

                    tweet_text = title
                    if description:
                        tweet_text += "\n" + description

                    logging.info(f"ğŸ†• æ£€æµ‹åˆ°æ–°æ¨æ–‡: {title or '[æ— æ ‡é¢˜]'}")
                    result = self.analyze_with_model(tweet_text)

                    if result:
                        logging.info(f"âœ… åˆ†æç»“æœ: {result}")
                        self.save_analysis_record(tweet_text, result)

                        # ---- ç­–ç•¥åŒ–è§¦å‘æ¨é€ ----
                        imp = str(result.get("importance", "none")).lower().strip()
                        impact_flag = bool(result.get("impact") is True)
                        threshold = IMPORTANCE_ORDER.get(self.push_min_importance, 3)
                        rank = IMPORTANCE_ORDER.get(imp, 0)
                        should_push = (not self.push_on_impact or impact_flag) and (rank >= threshold)

                        if should_push:
                            asset = result.get("asset_class")
                            asset_str = ", ".join(asset) if isinstance(asset, list) else str(asset or "")
                            link_line = f"åŸæ–‡é“¾æ¥ï¼š{link}" if link else ""
                            msg = self.push_alert_template.format(
                                original_cn=result.get("original_cn", ""),
                                asset_str=asset_str,
                                reason=result.get("reason", ""),
                                link_line=link_line
                            )
                            self.push_with_rate_limit(msg)
                    else:
                        # ---- æ— JSON å›é€€ï¼šé™„åŸæ–‡é“¾æ¥ + å¿«é€Ÿç¿»è¯‘ ----
                        logging.warning("âš ï¸ æ¨¡å‹æœªè¿”å›æœ‰æ•ˆ JSONï¼Œæ‰§è¡Œå›é€€æ¨é€")
                        brief_cn = self.quick_translate_cn(tweet_text) or "(ç¿»è¯‘å¤±è´¥)"
                        link_line = f"åŸæ–‡é“¾æ¥ï¼š{link}" if link else ""
                        fallback_msg = self.push_fallback_template.format(brief_cn=brief_cn, link_line=link_line)
                        self.push_with_rate_limit(fallback_msg)
                        # ä¹Ÿå†™ä¸€æ¡â€œç©ºç»“æœâ€çš„è®°å½•
                        self.save_analysis_record(tweet_text, None)

                    # æ ‡è®°ä¸ºå·²åˆ†æï¼ˆç”¨ç¨³å®šæŒ‡çº¹ï¼‰
                    self.analyzed_guids.add(fp)
                    self.save_analyzed_guids()

                    # ---- æ¡ç›®é—´æœ€å°é—´éš” ----
                    if self.min_sleep_between_entries_ms > 0:
                        time.sleep(self.min_sleep_between_entries_ms / 1000.0)

                logging.info("âœ… æœ¬è½® RSS å¤„ç†ç»“æŸï¼Œè¿›å…¥ä¼‘çœ ")
                time.sleep(self.check_interval)
            except Exception as e:
                logging.error(f"è¿è¡Œæ—¶é”™è¯¯: {e}")
                time.sleep(self.check_interval)

# =========================
# å…¥å£
# =========================
if __name__ == '__main__':
    monitor = TrumpTweetMonitor()
    monitor.run()
